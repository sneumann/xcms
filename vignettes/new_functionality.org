#+TITLE: New and modified functionality in xcms
#+AUTHOR:    Johannes Rainer
#+EMAIL:     johannes.rainer@eurac.edu
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS: ^:{} toc:nil
#+PROPERTY: exports code
#+PROPERTY: session *R*

#+BEGIN_EXPORT html
---
title: "New and modified functionality in xcms"
author: "Johannes Rainer"
graphics: yes
package: xcms
output:
  BiocStyle::html_document2:
    toc_float: true
vignette: >
  %\VignetteIndexEntry{New and modified functionality in xcms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{xcms,RColorBrewer}
bibliography: references.bib
csl: biomed-central.csl
references:
- id: dummy
  title: no title
  author:
  - family: noname
    given: noname
---

#+END_EXPORT

* New functionality in =xcms=

This document describes new functionality and changes to existing functionality
in the =xcms= package introduced during the update to version /3/.

#+BEGIN_SRC R :ravel message = FALSE, warning = FALSE
  library(xcms)
  library(RColorBrewer)
  register(bpstart(MulticoreParam()))
#+END_SRC

** Modernized user interface

The modernization of the user interface comprises new classes for data
representation and new data analysis methods. In addition, the core logic for
the data processing has been extracted from the old methods and put into a set
of R functions, the so called core API functions (or =do_= functions). These
functions take standard R data structures as input and return standard R data
types as result and can hence be easily included in other R packages.

The new user interface aims at simplifying and streamlining the =xcms= workflow
while guaranteeing data integrity and performance also for large scale
metabolomics experiments. Importantly, a simplified access to the original raw
data should be provided throughout the whole metabolomics data analysis workflow.

# All objects in the new user interface ensuring
# data integrity /via/ validation methods and class versioning, all methods are
# tested internally in extensive unit tests to guarantee proper functionality.

The new interface re-uses objects from the =MSnbase= Bioconductor package, such as
the =OnDiskMSnExp= object. This object is specifically designed for large scale MS
experiments as it initially reads just the scan header information from the mzML
while the mz-intensity value pairs from all or from selected spectra of a file
are read on demand hence minimizing the memory demand. Also, in contrast to
the old =xcmsRaw= object, the =OnDiskMSnExp= contains information from all files of
an experiment. In addition, all data normalization and adjustment methods
implemented in the =MSnbase= package can be directly applied to the MS data
without the need to re-implement such methods in =xcms=. Results from =xcms=
preprocessings, such as chromatographic peak detection or correspondence are
stored into the new =XCMSnExp= object. This object extends the =OnDiskMSnExp= object
and inherits thus all of its methods including raw data access.

Class and method/function names follow also a new naming convention trying tp
avoid the partially confusing nomenclature of the original =xcms= methods (such as
the =group= method to perform the correspondence of peaks across samples). To
distinguish them from mass peaks, the peaks identified by the peak detection in
an LS/GC-MS experiment are referred to as /chromatographic peaks/. The respective
method to identify such peaks is hence called =findChromPeaks= and the identified
peaks can be accessed using the =XCMSnExp= =chromPeaks= method. The results from an
correspondence analysis which aims to match and group chromatographic peaks
within and between samples are called /features/. A feature corresponds to
individual ions with a unique mass-to-charge ratio (mz) and a unique retention
time (rt). The definition of such mz-rt features (i.e. the result from the
=groupChromPeaks= method) can be accessed /via/ the =featureDefinitions= method of
the =XCMSnExp= class. Finally, alignment (retention time correction) can be
performed using the =adjustRtime= method.

The settings for any of the new analysis methods are bundled in /parameter/
classes, one class for each method. This encapsulation of the parameters to a
function into a parameter class (such as =CentWaveParam=) avoids busy function
calls (with many single parameters) and enables saving, reloading and reusing
the settings. In addition, the parameter classes are added, along with other
information to the process history of an =XCMSnExp= object thus providing a
detailed documentation of each processing step of an analysis, with the
possibility to recall all settings of the performed analyses at any stage. In
addition, validation of the parameters can be performed within the parameter
object and hence is no longer required in the analysis function.

The example below illustrates the new user interface. First we load the raw data
files from the =faahKO= package using the =readMSData2= from the =MSnbase= package.

#+BEGIN_SRC R :ravel message = FALSE, warning = FALSE
  ## Reading the raw data using the MSnbase package
  library(xcms)
  ## Load 6 of the CDF files from the faahKO
  cdf_files <- dir(system.file("cdf", package = "faahKO"), recursive = TRUE,
                   full.names = TRUE)[c(1:3, 7:9)]

  ## Define the sample grouping.
  s_groups <- rep("KO", length(cdf_files))
  s_groups[grep(cdf_files, pattern = "WT")] <- "WT"
  ## Define a data.frame that will be used as phenodata
  pheno <- data.frame(sample_name = sub(basename(cdf_files), pattern = ".CDF",
					replacement = "", fixed = TRUE),
                      sample_group = s_groups, stringsAsFactors = FALSE)

  ## Read the data.
  raw_data <- readMSData2(cdf_files, pdata = new("NAnnotatedDataFrame", pheno))
#+END_SRC

We next plot the total ion chromatogram (TIC) for all files within the
experiment. Note that we are iteratively sub-setting the full data per file
using the =filterFile= method, which, for =OnDiskMSnExp= objects, is an efficient
way to subset the data while ensuring that all data, including metadata, stays
consistent.

#+NAME: faahKO-tic
#+BEGIN_SRC R :ravel message = FALSE, fig.align = 'center', fig.width = 8, fig.height = 4
  library(RColorBrewer)
  sample_colors <- brewer.pal(3, "Set1")[1:2]
  names(sample_colors) <- c("KO", "WT")
  ## Subset the full raw data by file and plot the data.
  tmp <- filterFile(raw_data, file = 1)
  plot(x = rtime(tmp), y = tic(tmp), xlab = "retention time", ylab = "TIC",
       col = paste0(sample_colors[pData(tmp)$sample_group], 80), type = "l")
  for (i in 2:length(fileNames(raw_data))) {
      tmp <- filterFile(raw_data, file = i)
      points(rtime(tmp), tic(tmp), type = "l",
             col = paste0(sample_colors[pData(tmp)$sample_group], 80))
  }
  legend("topleft", col = sample_colors, legend = names(sample_colors), lty = 1)
#+END_SRC

Alternatively we can use the =extractChromatograms= method that extracts
chromatograms from the object. In the example below we extract the /base peak
chromatogram/ (BPC) by setting =aggregationFun= to ="max"= and not specifying an =rt=
or =mz= range to extract only a data subset. In contrast to the =tic= and =bpi=
methods, this function reads the data from the raw files. It takes thus more
time to create the plot, but it is based on the actual raw data that is used for
the later analysis - the =tic= and =bpi= methods access only the information that is
stored in the raw data files by the MS detector during the data acquisition.

#+NAME: faahKO-bpi
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 4
  ## Get the base peak chromatograms. This reads data from the files.
  bpis <- extractChromatograms(raw_data, aggregationFun = "max")
  ## Plot the list of Chromatogram objects.
  plotChromatogram(bpis, col = paste0(sample_colors[pData(raw_data)$sample_group], 80))

#+END_SRC

While the =plotChromatogram= function if very convenient (and fast), it would also
not be too difficult to create the plot manually:

#+NAME: faahKO-bbpi-manual
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 4
  plot(3, 3, pch = NA, xlim = range(unlist(lapply(bpis, rtime))),
       ylim = range(unlist(lapply(bpis, intensity))), main = "BPC",
       xlab = "rtime", ylab = "intensity")
  for (i in 1:length(bpis)) {
      points(rtime(bpis[[i]]), intensity(bpis[[i]]), type = "l",
             col = paste0(sample_colors[pData(raw_data)$sample_group[i]], 80))
  }
#+END_SRC


Note that we could restrict the analysis to a certain retention time range by
first sub-setting =raw_data= with the =filterRt= method.

In addition we can plot the distribution of the total ion counts per file. In
contrast to sub-setting the object we split the numeric vector returned by the
=tic= by file using the =fromFile= method that provides the mapping of the
experiment's spectra to the originating files.

#+NAME: faahKO-tic-boxplot
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 4
  ## Get the total ion current by file
  tc <- split(tic(raw_data), f = fromFile(raw_data))
  boxplot(tc, col = paste0(sample_colors[pData(raw_data)$sample_group], 80),
          ylab = "intensity", main = "Total ion current")
#+END_SRC

The =tic= (and for mzML files) the =bpi= methods are very fast, even for large data
sets, as these information are stored in the header of the raw files avoiding
the need to read the raw data from each file. Also, we could subset the whole
object using the filter functions =filterFile=, =filterRt= or =filterMz= to
e.g. remove problematic samples or restrict the retention time range in which we
want to perform the chromatographic peak detection.

Next we perform the chromatographic peak detection using the /centWave/ algorithm
\cite{Tautenhahn:2008fx}. In the example below we use most of the standard
parameters, but the settings should be adjusted to each experiment individually
based on e.g. the expected width of the chromatographic peaks etc.

#+NAME: faahKO-centWave
#+BEGIN_SRC R :ravel message = FALSE, warning = FALSE
  ## Defining the settings for the centWave peak detection.
  cwp <- CentWaveParam(snthresh = 20, noise = 1000)
  xod <- findChromPeaks(raw_data, param = cwp)
#+END_SRC

The identified peaks can be accessed with the =chromPeaks= parameter which returns
a =matrix=, each line representing an identified peak. Column ="sample"= specifies
in which /sample/ (i.e. file) of the experiment the peak was detected. Below we
plot the signal distribution of the identified peaks per sample.

#+NAME: faahKO-peak-intensity-boxplot
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 4
  ints <- split(chromPeaks(xod)[, "into"], f = chromPeaks(xod)[, "sample"])
  ints <- lapply(ints, log2)
  boxplot(ints, varwidth = TRUE, col = sample_colors[pData(xod)$sample_group],
          ylab = expression(log[2]~intensity), main = "Peak intensities")
#+END_SRC

After peak detection it might be advisable to evaluate whether the peak
detection identified e.g. compounds known to be present in the
sample. Facilitating access to the raw data has thus been one of the major aims
for the updated user interface.

Next we extract the chromatogram for the rt-mz region corresponding to one
detected chromatographic peak increasing the region in rt dimension by +/- 60
seconds. In addition we extract also the full chromatogram for the specified mz
range (i.e. the full rt range) and identify all chromatographic peaks in that
region by passing the same =mz= and =rt= parameters to the =chromPeaks= method.

If two-column matrices are passed to the =extractChromatograms= method with
parameters =rt= and =mz=, the function returns a =list=, each element being a =list= of
=Chromatogram= objects representing the chromatogram for the respective
ranges.

#+NAME: faahKO-chromPeaks-extractChroms
#+BEGIN_SRC R :ravel warning = FALSE
  rtr <- chromPeaks(xod)[68, c("rtmin", "rtmax")]
  ## Increase the range:
  rtr[1] <- rtr[1] - 60
  rtr[2] <- rtr[2] + 60
  mzr <- chromPeaks(xod)[68, c("mzmin", "mzmax")]

  ## Add an rt range that would extract the full chromatogram
  rtr <- rbind(c(-Inf, Inf), rtr)
  mzr <- rbind(mzr, mzr)

  chrs <- extractChromatograms(xod, rt = rtr, mz = mzr)

  ## In addition we get all peaks detected in the same region
  pks <- chromPeaks(xod, rt = rtr, mz = mzr)
  pks
#+END_SRC 

Next we plot the extracted chromatogram for the data and highlight in addition
the identified peaks.

#+NAME: faahKO-extracted-chrom-with-peaks
#+BEGIN_SRC R :ravel message = FALSE, fig.cap = "Extracted ion chromatogram for one of the identified peaks. Left: full retention time range, right: rt range of the peak. Each line represents the signal measured in one sample. The rectangles indicate the margins of the identified chromatographic peak in the respective sample.", fig.align = "center", fig.width = 12, fig.height = 6
  ## Plot the full rt range:
  plotChromatogram(chrs[[1]],
                   col = paste0(sample_colors[pData(xod)$sample_group], 80))
  ## And now for the peak range.
  plotChromatogram(chrs[[2]],
                   col = paste0(sample_colors[pData(xod)$sample_group], 80))
  ## Highlight also the identified chromatographic peaks.
  highlightChromPeaks(xod, rt = rtr[2, ], mzr[2, ],
                      border = paste0(sample_colors[pData(xod)$sample_group], 40))
#+END_SRC

Note that the =extractChromatograms= does return an =NA= value if in a certain scan
(i.e. for a specific retention time) no signal was measured in the respective mz
range. This is reflected by the lines not being drawn as continuous lines in the
plot above.

Next we align the samples using the /obiwarp/ method \cite{Prince:2006jj}. This
method does not require, in contrast to other alignment/retention time
correction methods, any identified peaks and could thus also be applied to an
=OnDiskMSnExp= object. Note that all retention time adjustment methods do also
adjust the retention times reported for the individual peaks in =chromPeaks=.

#+NAME: faahKO-obiwarp
#+BEGIN_SRC R :ravel message = FALSE
  ## Doing the obiwarp alignment using the default settings.
  xod <- adjustRtime(xod, param = ObiwarpParam())
#+END_SRC

Note that any pre-processing results can be removed at any time using a /drop/
method, such as =dropChromPeaks=, =dropFeatureDefinitions= or
=dropAdjustedRtime=.

To evaluate the impact of the alignment we can plot again the BPC of each
sample. In addition we plot the differences of the adjusted to the raw retention
times per sample using the =plotAdjustedRtime= function.

#+NAME: faahKO-bpi-obiwarp
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 8
  ## Get the base peak chromatograms. This reads data from the files.
  bpis <- extractChromatograms(xod, aggregationFun = "max")

  par(mfrow = c(2, 1), mar = c(4.5, 4.2, 1, 0.5))
  plotChromatogram(bpis,
                   col = paste0(sample_colors[pData(xod)$sample_group[i]], 80))
  ## Plot also the difference of adjusted to raw retention time.
  plotAdjustedRtime(xod, col = paste0(sample_colors[pData(xod)$sample_group], 80))
#+END_SRC

Too large differences between adjusted and raw retention times could indicate
poorly performing samples or alignment.

The distribution of retention time differences could also be used for quality
assessment.

#+NAME: faahKO-adjusted-rtime-boxplot
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 4
  ## Calculate the difference between the adjusted and the raw retention times.
  diffRt <- rtime(xod) - rtime(xod, adjusted = FALSE)

  ## By default, rtime and most other accessor methods return a numeric vector. To
  ## get the values grouped by sample we have to split this vector by file/sample
  diffRt <- split(diffRt, fromFile(xod))

  boxplot(diffRt, col = sample_colors[pData(xod)$sample_group],
          main = "Obiwarp alignment results", ylab = "adjusted - raw rt")
#+END_SRC

The 3rd sample was used as /center/ sample against which all other samples were
aligned to, hence its adjusted retention times are identical to the raw
retention times.

Below we plot the extracted ion chromatogram for the selected peak from the
example above before and after retention time correction to evaluate the impact
of the alignment.

#+NAME: faahKO-extracted-chrom-with-peaks-aligned
#+BEGIN_SRC R :ravel echo = FALSE, message = FALSE, fig.cap = "Extracted ion chromatogram for one of the identified peaks before and after alignment.", fig.align = "center", fig.width = 8, fig.height = 8
  rtr <- chromPeaks(xod)[68, c("rtmin", "rtmax")]
  ## Increase the range:
  rtr[1] <- rtr[1] - 60
  rtr[2] <- rtr[2] + 60
  mzr <- chromPeaks(xod)[68, c("mzmin", "mzmax")]

  chrs <- extractChromatograms(xod, rt = rtr, mz = mzr)
  chrs_raw <- extractChromatograms(raw_data, rt = rtr, mz = mzr)

  par(mfrow = c(2, 1))
  plotChromatogram(chrs_raw,
                   col = paste0(sample_colors[pData(xod)$sample_group], 80))
  plotChromatogram(chrs,
                   col = paste0(sample_colors[pData(xod)$sample_group], 80))
  highlightChromPeaks(xod, rt = rtr, mzr,
                      border = paste0(sample_colors[pData(xod)$sample_group], 40))
#+END_SRC

After alignment, the peaks are nicely overlapping.

Next we group identified chromatographic peaks across samples. We use the /peak
density/ method \cite{Smith:2006ic} specifying that a chromatographic peak have
to be present in at least 1/3 of the samples within each group to be combined to
a mz-rt /feature/.

#+NAME: faahKO-groupPeakDensity
#+BEGIN_SRC R :ravel message = FALSE
  ## Define the PeakDensityParam
  pdp <- PeakDensityParam(sampleGroups = pData(xod)$sample_group,
                          maxFeatures = 300, minFraction = 0.66)
  xod <- groupChromPeaks(xod, param = pdp)
#+END_SRC

The definitions of the features can be accessed with the =featureDefinitions=,
which lists the mz-rt space specific to a feature. Column ="peakidx"= lists the
indices (in the =chromPeaks= matrix) of the individual chromatographic peaks
belonging to the feature.

#+NAME: faahKO-featureDefinitions
#+BEGIN_SRC R :ravel message = FALSE
  head(featureDefinitions(xod))
#+END_SRC

To extract /values/ for the features, the =featureValues= method can be used. This
method returns a matrix with rows being the features and column the samples. The
=value= parameter allows to specify the value that should be returned. Below we
extract the ="into"= signal, i.e. the per-peak integrated intensity for each
feature.

#+NAME: faahKO-featureValues
#+BEGIN_SRC R :ravel message = FALSE
  ## Extract the "into" peak integrated signal.
  head(featureValues(xod, value = "into"))
#+END_SRC

After correspondence there will always be features that do not include peaks
from every sample (being it that the peak finding algorithm failed to identify a
peak or that no signal was measured in the respective mz-rt area). For such
features an =NA= is returned by the =featureValues= method. Here, =xcms= allows to
infer values for such missing peaks using the =fillChromPeaks= method. This method
integrates in files where a peak was not found the signal from the mz-rt area
where it is expected and adds it to the =chromPeaks= matrix. Such /filled-in/ peaks
have a value of =1= in the ="is_filled"= column of the =chromPeaks= matrix.

#+NAME: faahKO-fillPeaks
#+BEGIN_SRC R :ravel message = FALSE
  ## Fill in peaks with default settings. Settings can be adjusted by passing
  ## a FillChromPeaksParam object to the method.
  xod <- fillChromPeaks(xod)

  head(featureValues(xod, value = "into"))
#+END_SRC

Not for all missing peaks a value could be integrated (because at the respective
location no measurements are available). The peak area from which signal is to
be extracted can also be increased modifying the settings by passing a
=FillChromPeaksParam= object.

Next we inspect the =processHistory= of the analysis. As described earlier, this
records all (major) processing steps along with the corresponding parameter
classes.

#+NAME: faahKO-processHistory
#+BEGIN_SRC R :ravel message = FALSE
  ## List the full process history
  processHistory(xod)
#+END_SRC

It is also possible to extract specific processing steps by specifying its
type. Available types can be listed with the =processHistoryTypes= function. Below
we extract the parameter class for the alignment/retention time adjustment step.

#+NAME: faahKO-processHistory-select
#+BEGIN_SRC R :ravel message = FALSE
  ph <- processHistory(xod, type = "Retention time correction")

  ## Access the parameter
  processParam(ph[[1]])
#+END_SRC

As described earlier, we can remove specific analysis results at any
stage. Below we remove the results from the alignment. Since the correspondence
was performed after that processing step its results will be removed too leaving
us only with the results from the peak detection step.

#+NAME: faahKO-drop-alignment
#+BEGIN_SRC R :ravel message = FALSE
  ## Remove the alignment results
  xod <- dropAdjustedRtime(xod)

  processHistory(xod)
#+END_SRC

We can now use a different method to perform the alignment. The /peak groups/
alignment method bases the alignment of the samples on chromatographic peaks
present in most samples (so called /well behaved/ peaks). This means we have to
perform first an initial correspondence analysis to group peaks within and
across samples.

#+NAME: faahKO-initial-correspondence
#+BEGIN_SRC R :ravel message = FALSE
  ## Define the parameter for the correspondence
  pdparam <- PeakDensityParam(sampleGroups = pData(xod)$sample_group,
                              minFraction = 0.7, maxFeatures = 100)
  xod <- groupChromPeaks(xod, param = pdparam)
#+END_SRC

Before performing the alignment we can also inspect which peak groups might be
selected for alignment based on the provided =PeakGroupsParam= object.

#+NAME: faahKO-peak-groups-matrix
#+BEGIN_SRC R :ravel message = FALSE
  ## Create the parameter class for the alignment
  pgparam <- PeakGroupsParam(minFraction = 0.9, span = 0.4)

  ## Extract the matrix with (raw) retention times for the peak groups that would
  ## be used for alignment.
  adjustRtimePeakGroups(xod, param = pgparam)
#+END_SRC

If we are not happy with these peak groups (e.g. because we don't have a peak
group for a rather large time span along the retention time axis) we can try
different settings. In addition, we could also /manually/ select certain peak
groups, e.g. for internal controls, and add this matrix with the
=peakGroupsMatrix= method to the =PeakGroupsParam= class. Below we just use =pgparam=
we defined and perform the alignment. This will use the peak groups matrix from
above.

#+NAME: faahKO-peak-groups-alignment
#+BEGIN_SRC R :ravel message = FALSE
  ## Perform the alignment using the peak groups method.
  xod <- adjustRtime(xod, param = pgparam)
#+END_SRC

We can now also plot the difference between adjusted and raw retention times. If
alignment was performed using the /peak groups/ method, also these peak groups are
highlighted in the plot.

#+NAME: faahKO-peak-groups-alignment-plot
#+BEGIN_SRC R :ravel message = FALSE, fig.align = "center", fig.width = 8, fig.height = 4
  plotAdjustedRtime(xod, col = sample_colors[pData(xod)$sample_group])
#+END_SRC

** New naming convention

Peaks identified in LC/GC-MS metabolomics are referred to as /chromatographic
peaks/ where possible to avoid any misconceptions with /mass peaks/ identified in
mz dimension.

Methods for data analysis from the original =xcms= code have been renamed to avoid
potential confusions:

+ *Chromatographic peak detection*: =findChromPeaks= instead of =findPeaks=: for new
  functions and methods the term /peak/ is avoided as much as possible, as it is
  usually used to describe a mass peak in mz dimension. To clearly distinguish
  between these peaks and peaks in retention time space, the latter are referred
  to as /chromatographic peak/, or =chromPeak=.

+ *Correspondence*: =groupChromPeaks= instead of =group= to clearly indicate what is
  being grouped. Group might be a sample group or a peak group, the latter being
  referred to also by (mz-rt) /feature/.
  
+ *Alignment*: =adjustRtime= instead of =retcor= for retention time correction. The
  word /cor/ in /retcor/ might be easily misinterpreted as /correlation/ instead of
  correction.


** New data classes

*** =OnDiskMSnExp=

This object is defined and documented in the =MSnbase= package. In brief, it is a
container for the full raw data from an MS-based experiment. To keep the memory
footprint low the mz and intensity values are only loaded from the raw data
files when required. The =OnDiskMSnExp= object replaces the =xcmsRaw= object.

*** =XCMSnExp=

The =XCMSnExp= class extends the =OnDiskMSnExp= object from the =MSnbase= package and
represents a container for the xcms-based preprocessing results while (since it
inherits all functionality from its parent class) keeping a direct relation to
the (raw) data on which the processing was performed. An additional slot
=.processHistory= in the object allows to keep track of all performed processing
steps. Each analysis method, such as =findChromPeaks= adds an =XProcessHistory=
object which includes also the parameter class passed to the analysis
method. Hence not only the time and type of the analysis, but its exact settings
are reported within the =XCMSnExp= object. The =XCMSnExp= is thus equivalent to the
=xcmsSet= from the original =xcms= implementation, but keeps in addition a link to
the raw data on which the preprocessing was performed.

*** =Chromatogram=

The =Chromatogram= class allows a data representation that is orthogonal to the
=Spectrum= class defined in =MSnbase=. The =Chromatogram= class stores retention time
and intensity duplets and is designed to accommodate most use cases, from total
ion chromatogram, base peak chromatogram to extracted ion chromatogram and
SRM/MRM ion traces.

=Chromatogram= objects can be extracted from =XCMSnExp= objects using the
=extractChromatograms= method.

Note that this class is still considered developmental and might thus undergo
some changes in the future.

** Binning and missing value imputation functions

The binning/profile matrix generation functions have been completely
rewritten. The new =binYonX= function replaces the binning of intensity values
into bins defined by their m/z values implemented in the =profBin=, =profBinLin= and
=profBinLinBase= methods. The =binYonX= function provides also additional functionality:

+ Breaks for the bins can be defined based on either the number of desired bins
  (=nBins=) or the size of a bin (=binSize=). In addition it is possible to provide
  a vector with pre-defined breaks. This allows to bin data from multiple files
  or scans on the same bin-definition.

+ The function returns a list with element =y= containing the binned values and
  element =x= the bin mid-points.

+ Values in input vector =y= can be aggregated within each bin with different
  methods: =max=, =min=, =sum= and =mean=.

+ The index of the largest (or smallest for =method= being "min") within each bin
  can be returned by setting argument =returnIndex= to =TRUE=.

+ Binning can be performed on single or multiple sub-sets of the input vectors
  using the =fromIdx= and =toIdx= arguments. This replaces the /M/ methods (such as
  =profBinM=). These sub-sets can be overlapping.

The missing value imputation logic inherently build into the =profBinLin= and
=profBinLinBase= methods has been implemented in the =imputeLinInterpol= function.

The example below illustrates the binning and imputation with the =binYtoX= and
=imputeLinInterpol= functions. After binning of the test vectors below some of the
bins have missing values, for which we impute a value using
=imputeLinInterpol=. By default, =binYonX= selects the largest value within each
bin, but other aggregation methods are also available (i.e. min, max, mean,
sum).

#+BEGIN_SRC R :ravel message = FALSE
  ## Defining the variables:
  set.seed(123)
  X <- sort(abs(rnorm(30, mean = 20, sd = 25))) ## 10
  Y <- abs(rnorm(30, mean = 50, sd = 30))

  ## Bin the values in Y into 20 bins defined on X
  res <- binYonX(X, Y, nBins = 22)

  res
#+END_SRC

As a result we get a =list= with the bin mid-points (=$x=) and the binned =y= values
(=$y=).

Next we use two different imputation approaches, a simple linear interpolation
and the linear imputation approach that was defined in the =profBinLinBase=
method. The latter performs linear interpolation only considering a certain
neighborhood of missing values otherwise replacing the =NA= with a base value.

#+BEGIN_SRC R :ravel binning-imputation-example, message = FALSE, fig.width = 10, fig.height = 7, fig.cap = 'Binning and missing value imputation results. Black points represent the input values, red the results from the binning and blue and green the results from the imputation (with method lin and linbase, respectively).'
  ## Plot the actual data values.
  plot(X, Y, pch = 16, ylim = c(0, max(Y)))
  ## Visualizing the bins
  abline(v = breaks_on_nBins(min(X), max(X), nBins = 22), col = "grey")

  ## Define colors:
  point_colors <- paste0(brewer.pal(4, "Set1"), 80)
  ## Plot the binned values.
  points(x = res$x, y = res$y, col = point_colors[1], pch = 15)

  ## Perform the linear imputation.
  res_lin <- imputeLinInterpol(res$y)

  points(x = res$x, y = res_lin, col = point_colors[2], type = "b")

  ## Perform the linear imputation "linbase"
  res_linbase <- imputeLinInterpol(res$y, method = "linbase")
  points(x = res$x, y = res_linbase, col = point_colors[3], type = "b", lty = 2)
#+END_SRC

The difference between the linear interpolation method =lin= and =linbase= is that
the latter only performs the linear interpolation in a pre-defined neighborhood
of the bin with the missing value (=1= by default). The other missing values are
set to a base value corresponding to half of the smallest bin value. Both
methods thus yield same results, except for bins 15-17 (see Figure above).

** Core functionality exposed /via/ simple functions

The core logic from the chromatographic peak detection methods
=findPeaks.centWave=, =findPeaks.massifquant=, =findPeaks.matchedFilter= and
=findPeaks.MSW= and from all alignment (=group.*=) and correspondence (=retcor.*=)
methods has been extracted and put into functions with the common prefix
=do_findChromPeaks=, =do_adjustRtime= and =do_groupChromPeaks=, respectively, with the
aim, as detailed in issue [[https://github.com/sneumann/xcms/issues/30][#30]], to separate the core logic from the analysis
methods invoked by the users to enable also the use these methods using base R
parameters (i.e. without specific classes containing the data such as the
=xcmsRaw= class). This simplifies also the re-use of these functions in other
packages and simplifies the future implementation of the peak detection
algorithms for e.g. the =MSnExp= or =OnDiskMSnExp= objects from the =MSnbase=
Bioconductor package. The implemented functions are:

+ *peak detection methods*:
  + =do_findChromPeaks_centWave=: peak density and wavelet based peak detection
    for high resolution LC/MS data in centroid mode \cite{Tautenhahn:2008fx}.
  + =do_findChromPeaks_matchedFilter=: identification of peak in the
    chromatographic domain based on matched filtration \cite{Smith:2006ic}.
  + =do_findChromPeaks_massifquant=: identification of peaks using Kalman
    filters.
  + =do_findChromPeaks_MSW=: single spectrum, non-chromatographic peak detection.

+ *alignment methods*:
  + =do_adjustRtime_peakGroups=: perform sample alignment (retention time
    correction) using alignment of /well behaved/ chromatographic peaks that are
    present in most samples (and are expected to have the same retention time).

+ *correspondence methods*:
  + =do_groupChromPeaks_density=: perform chromatographic peak grouping (within
    and across samples) based on the density distribution of peaks along the
    retention time axis.
  + =do_groupChromPeaks_nearest=: groups peaks across samples similar to the
    method implemented in mzMine.
  + =do_groupChromPeaks_mzClust=: performs high resolution correspondence on
    single spectra samples.

One possible drawback from the introduction of this new layer is, that more
objects get copied by R which /could/ eventually result in a larger memory demand
or performance decrease (while no such was decrease was observed up to now).

** Usability improvements in the /old/ user interface

+ =[= subsetting method for =xcmsRaw= objects that enables to subset an =xcmsRaw=
  object to specific scans/spectra.
+ =profMat= method to extract the /profile/ matrix from the =xcmsRaw= object. This
  method should be used instead of directly accessing the =@env$profile= slot, as
  it will create the profile matrix on the fly if it was not pre-calculated (or
  if profile matrix generation settings have been changed).

* Changes due to bug fixes and modified functionality

** Differences in linear interpolation of missing values (=profBinLin=).

From =xcms= version 1.51.1 on the new binning functions are used, thus, the bug
described here are fixed.

Two bugs are present in the =profBinLin= method (reported as issues [[https://github.com/sneumann/xcms/issues/46][#46]] and [[https://github.com/sneumann/xcms/issues/49][#49]] on
github) which are fixed in the new =binYonX= and =imputeLinInterpol= functions:

+ The first bin value calculated by =profBinLin= can be wrong (i.e. not being the
  max value within that bin, but the first).
+ If the last bin contains also missing values, the method fails to determine
  a correct value for that bin.

The =profBinLin= method is used in =findPeaks.matchedFilter= if the profile
method is set to "binlin".

The example below illustrates both differences.

#+BEGIN_SRC R
  ## Define a vector with empty values at the end.
  X <- 1:11
  set.seed(123)
  Y <- sort(rnorm(11, mean = 20, sd = 10))
  Y[9:11] <- NA
  nas <- is.na(Y)
  ## Do interpolation with profBinLin:
  resX <- xcms:::profBinLin(X[!nas], Y[!nas], 5, xstart = min(X),
                            xend = max(X))
  resX
  res <- binYonX(X, Y, nBins = 5L, shiftByHalfBinSize = TRUE)
  resM <- imputeLinInterpol(res$y, method = "lin",
                            noInterpolAtEnds = TRUE)
  resM
#+END_SRC

Plotting the results helps to better compare the differences. The black points
in the figure below represent the actual values of =Y= and the grey vertical lines
the breaks defining the bins. The blue lines and points represent the result
from the =profBinLin= method. The bin values for the first and 4th bin are clearly
wrong. The green colored points and lines represent the results from the =binYonX=
and =imputeLinInterpol= functions (showing the correct binning and interpolation).

#+BEGIN_SRC R :ravel profBinLin-problems, message = FALSE, fig.align = 'center', fig.width=10, fig.height = 7, fig.cap = "Illustration of the two bugs in profBinLin. The input values are represented by black points, grey vertical lines indicate the bins. The results from binning and interpolation with profBinLin are shown in blue and those from binYonX in combination with imputeLinInterpol in green."
  plot(x = X, y = Y, pch = 16, ylim = c(0, max(Y, na.rm = TRUE)),
       xlim = c(0, 12))
  ## Plot the breaks
  abline(v = breaks_on_nBins(min(X), max(X), 5L, TRUE), col = "grey")
  ## Result from profBinLin:
  points(x = res$x, y = resX, col = "blue", type = "b")
  ## Results from imputeLinInterpol
  points(x = res$x, y = resM, col = "green", type = "b",
         pch = 4, lty = 2)

#+END_SRC

Note that by default =imputeLinInterpol= would also interpolate missing values at
the beginning and the end of the provided numeric vector. This can be disabled
(to be compliant with =profBinLin=) by setting parameter =noInterpolAtEnds= to
=TRUE= (like in the example above).

** Differences due to updates in =do_findChromPeaks_matchedFilter=, respectively =findPeaks.matchedFilter=.

The original =findPeaks.matchedFilter= (up to version 1.49.7) had several
shortcomings and bugs that have been fixed in the new
=do_findChromPeaks_matchedFilter= method:

+ The internal iterative processing of smaller chunks of the full data (also
  referred to as /iterative buffering/) could result, for some bin (step) sizes to
  unstable binning results (discussed in issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github): calculation of
  the breaks, or to be precise, the actually used bin size was performed in each
  iteration and could lead to slightly different sizes between iterations (due
  to rounding errors caused by floating point number representations in C).

+ The iterative buffering raises also a conceptual issue when linear
  interpolation is performed to impute missing values: the linear imputation
  will only consider values within the actually processed buffer and can thus
  lead to wrong or inaccurate imputations.

+ The =profBinLin= implementation contains two bugs, one that can result in
  failing to identify the maximal value in the first and last bin (see issue
  [[https://github.com/sneumann/xcms/issues/46][#46]]) and one that fails to assign a value to a bin (issue [[https://github.com/sneumann/xcms/issues/49][#49]]). Both are fixed
  in the =do_findChromPeaks_matchedFilter= implementation.

A detailed description of tests comparing all implementations is available in
issue [[https://github.com/sneumann/xcms/issues/52][#52]] on github. Note also that in course of these changes also the =getEIC=
method has been updated to use the new binning and missing value imputation
function.

While it is strongly discouraged, it is still possible to use to /old/ code (from
1.49.7) by calling =useOriginalCode(TRUE)=.

** Differences in =findPeaks.massifquant=

+ Argument =scanrange= was ignored in the /original/ old code (issue [[https://github.com/sneumann/xcms/issues/61][#61]]).
+ The method returned a =matrix= if =withWave= was =0= and a =xcmsPeaks= object
  otherwise. The updated version returns *always* an =xcmsPeaks= object (issue #60).

** Differences in /obiwarp/ retention time correction

Retention time correction using the obiwarp method uses the /profile/ matrix
(i.e. intensities binned in discrete bins along the mz axis). Profile matrix
generation uses now the =binYonX= method which fixed some problems in the original
binning and linear interpolation methods. Thus results might be slightly
different.

Also, the =retcor.obiwarp= method reports (un-rounded) adjusted retention times,
but adjusts the retention time of eventually already identified peaks using
rounded adjusted retention times. The new =adjustRtime= method(s) does adjust
identified peaks using the reported adjusted retention times (not rounded). This
guarantees that e.g. removing retention time adjustment/alignment results from
an object restores the object to its initial state (i.e. the adjusted retention
times of the identified peaks are reverted to the retention times before
alignment).
See issue [[https://github.com/sneumann/xcms/issues/122][#122]] for more details.

** =retcor.peaksgroups=: change in the way how /well behaved/ peak groups are ordered

The =retcor.peakgroups= defines first the chromatographic peak groups that are
used for the alignment of all spectra. Once these are identified, the retention
time of the peak with the highest intensity in a sample for a given peak group
is returned and the peak groups are ordered increasingly by retention time
(which is required for the later fitting of either a polynomial or a linear
model to the data). The selection of the retention time of the peak with the
highest intensity within a feature (peak group) and samples, denoted as
/representative/ peak for a given feature in a sample, ensures that only the
retention time of a single peak per sample and feature is selected (note that
multiple chromatographic peaks within the same sample can be assigned to a
feature).  In the original code the ordering of the peak groups was however
performed using the median retention time of the complete peak group (which
includes also potential additional peaks per sample). This has been changed and
the features are ordered now by the median retention time across samples of the
representative chromatographic peaks.

** =scanrange= parameter in all =findPeaks= methods

The =scanrange= in the =findPeaks= methods is supposed to enable the peak detection
only within a user-defined range of scans. This was however not performed in
each method. Due to a bug in =findPeaks.matchedFilter='s original code the
argument was ignored, except if the upper scan number of the user defined range
was larger than the total number of available scans (see issue [[https://github.com/sneumann/xcms/issues/63][#63]]). In
=findPeaks.massifquant= the argument was completely ignored (see issue [[https://github.com/sneumann/xcms/issues/61][#61]]) and,
while the argument was considered in =findPeaks.centWave= and feature detection
was performed within the specified scan range, but the original =@scantime= slot
was used throughout the code instead of just the scan times for the specified
scan indices (see issue [[https://github.com/sneumann/xcms/issues/64][#64]]).

These problems have been fixed in version 1.51.1 by first sub-setting the
=xcmsRaw= object (using the =[= method) before actually performing the feature
detection.

** =fillPeaks= (=fillChromPeaks=) differences
   
In the original =fillPeaks.MSW=, the mz range from which the signal is to be
integrated was defined using 

#+BEGIN_SRC R :eval = "never", :ravel eval = FALSE
  mzarea <- seq(which.min(abs(mzs - peakArea[i, "mzmin"])),
		which.min(abs(mzs - peakArea[i, "mzmax"])))

#+END_SRC

Depending on the data this could lead to the inclusion of signal in the
integration that are just outside of the mz range. In the new =fillChromPeaks=
method signal is integrated only for mz values >= mzmin and <= mzmax thus
ensuring that only signal is used that is truly within the peak area defined by
columns ="mzmin"=, ="mzmax"=, ="rtmin"= and ="rtmax"=.

Also, the =fillPeaks.chrom= method did return ="into"= and ="maxo"= values of =0= if no
signal was found in the peak area. The new method does not integrate any signal
in such cases and does not fill in that peak.

See also issue [[https://github.com/sneumann/xcms/issues/130][#130]] for more
information.

** Problems with iterative binning of small data sub-sets in =findPeaks.matchedFilter= :noexport:

The problem described here has been fixed in =xcms= >= 1.51.1.

The iterative binning of only small sub-sets of data causes problems with
=profBinLinBase=, in which data imputation might be skipped in some iterations
while it is performed in others (also discussed in issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github).

Iterative buffering has both conceptual and computational issues.
+ Conceptual: =profBinLin= and =profBinLinBase= do a linear interpolation to impute
  missing values. This is obviously affected by the input data, i.e. if only a
  small subset of input data is considered, the imputation can change.

+ Computational: the iterative buffering is slower than binning of the full
  data.

An additional problem comes with the implementation of the =profBin= method in
=xcms= that was used in the =findPeaks.matchedFilter= method for method being =lin=:
the bin size is calculated anew in each call, thus, due to rounding errors
(imprecision of floating point numbers), the bin size will be slightly different
in each call, which can lead to wrong binning results (see issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github).

Example with =profBinLinBase= resulting in an error: if =step= and =basespace= are
both =0.1= it seems that not in all buffer-generation iterations a interpolation
is initiated, i.e. the variable =ibase= in the C-function is sometimes set to =1=
(interpolation with neighboring bins) and sometimes to =0=.

This is also extensively documented in issue [[https://github.com/sneumann/xcms/issues/52][#52]].

** Different binning results due to /internal/ and /external/ breaks definition :noexport:

*FIXED*: the bin calculation in C uses now also a multiplication instead of a
addition thus resulting in identical breaks!

Breaks calculated by the =breaks_on_nBins= function are equal as breaks calculated
using the =seq= function, but they are not identical.

#+BEGIN_SRC R
  library(xcms)

  ## Define breaks from 200 to 600
  brks <- seq(200, 600, length.out = 2002)
  brks2 <- xcms:::breaks_on_nBins(200, 600, nBins = 2001)
  all.equal(brks, brks2)
  identical(brks, brks2)

  ## The difference is very small, but could still, in the binning
  ## yield slightly different results depending on which breaks are
  ## used.
  range(brks - brks2)
#+END_SRC

** Implementation and comparison for =matchedFilter=		   :noexport:

These results base on the test =dontrun_test_do_findChromPeaks_matchedFilter_impl=
defined in /test_do_findChromPeaks_matchedFilter.R/

We have 4 different functions to test and compare to the original one:
+ *A*: =.matchedFilter_orig=: it's the original code.
+ *B*: =.matchedFilter_binYonX_iter=: uses the same sequential
  buffering than the original code, but uses =binYonX= for binning and
  =imputeLinInterpol= for interpolation.
+ *C*: =.matchedFilter_no_iter=: contains the original code, but
  avoids sequential buffering, i.e. creates the whole matrix in one go.
+ *D*: =.matchedFilter_binYonX_no_iter=: my favorite: uses =binYonX= and
  =imputeLinInterpol= and avoids the sequential buffering by creating the full
  matrix in one go.

Notes: for plain =bin= we expect that results with and without iterative buffering
are identical.

*Comparisons*:
+ [X] *A* /vs/ original:
  - =bin=: always OK.
  - =binlin=: always OK.
  - =binlinbase=: always OK.
+ [X] *B* /vs/ original:
  - =bin=: OK unless =step= is =0.2=: most likely rounding problem.
  - =binlin=: only once OK. Results are not equal, but comparable.
  - =binlinbase=: similar but not equal.
+ [X] *C* /vs/ original:
  - =bin=: OK unless =step= is =0.2=:
  - =binlin=: never OK: due to interpolation on full, or subset data.
  - =binlinbase=: similar but not equal.
+ [X] *D* /vs/ original:
  - =bin=: OK unless =step= is =0.2=: most likely rounding problem.
  - =binlin=: never OK: due to interpolation on full, or subset data AND due to
    fix of the bug in =profBinLin=.
  - =binlinbase=: similar but not equal.
+ [X] *B* /vs/ *C*:
  - =bin=: always OK.
  - =binlin=: results similar but not equal; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: highly similar.
+ [X] *B* /vs/ *D*:
  - =bin=: always OK.
  - =binlin=: results similar but not equal; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: highly similar.
+ [X] *C* /vs/ *D*:
  - =bin=: always OK.
  - =binlin=: results almost identical; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: always OK.


*Conclusions*:
+ =none= (only binning, but no linear interpolation; corresponds to method =bin= in
  =findPeaks.matchedFilter=): The results are identical between all methods for
  all except one setting: with =step= being =0.2= (or =0.4= etc) on one test file the
  results differ between methods with and without iterative buffering. The
  reason for this is most likely rounding errors in floating point number
  representation: =profBin= calculates the size of the bin in each call, thus,
  when called repeatedly based on different input values, the size is slightly
  different, which then can lead to binning differences (see also [[https://github.com/sneumann/xcms/issues/47][issue #47]] on
  github).

+ =lin= (binning followed by linear interpolation to impute missing values; method
  =binlin= in =findPeaks.matchedFilter=): There are two reasons for differences
  observed here: 1) the first bin value (and eventually the last bin value) are
  sometimes wrong (issue [[https://github.com/sneumann/xcms/issues/46][#46]]). This results in differences between =binYonX= and
  =imputeKinInterpol= based approach and =profBinLin= (with the former being
  presumably correct). Also, this has a bigger influence when the
  binning/missing value imputation is performed iteratively. Thus, the
  difference between the =binYonX= - =imputeLinInterpol= and =profBinLin= approach
  without iterative buffering are only very small. 2) Linear interpolation on
  the full data set compared to subsequent sub-sets will undoubtedly lead to
  differences. Because based on the full data set, the non-iterative approach
  results in the expected and more accurate results.

+ =linbase=: results are identical if =basespace= (respectively =distance=) is such
  that no interpolation takes place. With interpolation (e.g. =distance= being =1=)
  differences (albeit small) are present between approaches with and without iterative
  buffering. The results for the approaches without iterative buffering (using
  =profBinBase= respectively =binYonX= with =imputeLinIterpol=) are identical, again
  arguing in favor of these approaches.

Thus, summarizing, the approaches without the iterative buffering yield more
reliable (and presumably correct) results. Given also that the =binYonX= in
combination with =imputeLinInterpol= identify similar peaks than the non-iterative
approaches using the original code, we can change the code to use these former
methods as default.

* Under the hood changes

These changes and updates will not have any large impact on the day-to-day use of
=xcms= and are listed here for completeness.

+ From =xcms= version 1.51.1 on the default methods from the =mzR= package are used
  for data import. Besides ensuring easier maintenance, this enables also data
  import from /gzipped/ mzML files.


* Introducing =DRanges=.						   :noexport:

*Note*: the code for this is in the =dranges= branch. The last status/problem is
that it is not quite clear how to determine the /correct/ number of decimal
places: =as.character= uses =options()$scipen= to determine how many decimal places
are represented, =sprintf= allows much more decimal places, e.g. with =%.30f=, but
these become unstable and random. The /best/ solution for now would be to limit to
a certain number of /secure/ decimal places (16?) and specify this as global
option that might be changed later. Check also =.Machine= for details on
precision, max integer etc. Note also that we are pretty much limited by the
largest =integer= that can be represented.

The =multiplier= thus has definitely be smaller than:
#+BEGIN_SRC R
  maxPos <- nchar(as.character(.Machine$integer.max))
  maxMult <- 10^maxPos

#+END_SRC

Note that we would actually just have to check that the to-be-transformed
integers don't get too large; thus we could allow more decimal places.

The idea is to use all of the =IRanges= functionality, but for any =numeric=
ranges. Examples for such ranges could be the m/z range of a feature, or the
retention time range defining a feature.

The idea is pretty simple, the =DRanges= (/D/ standing for /double/, alternatively /N/
for /numeric/) extends the =IRanges=, the =start= and =end= of the =IRanges= are
calculated by multiplying the start and end defining the numeric range by =10^d=
with =d= being the number of decimal places.

First thing is to get the number of decimal places: using code from a pretty old
post on stackoverflow
(http://stackoverflow.com/questions/5173692/how-to-return-number-of-decimal-places-in-r):


#+BEGIN_SRC R
  decimalplaces <- function(x) {
      if ((x %% 1) != 0) {
          nchar(strsplit(sub('0+$', '', as.character(x)), ".", fixed=TRUE)[[1]][[2]])
      } else {
          return(0)
      }
  }

  num.decimals <- function(x) {
      stopifnot(class(x)=="numeric")
      x <- sub("0+$","",x)
      x <- sub("^.+[.]","",x)
      nchar(x)
  }


#+END_SRC

The former is actually faster.

Eventually even =C=?
http://stackoverflow.com/questions/1083304/c-c-counting-the-number-of-decimals

#+BEGIN_EXAMPLE
  string number = "543.014";
  size_t dotFound;
  stoi(number, &dotFound));
  string(number).substr(dotFound).size()
#+END_EXAMPLE

Be aware that =number= MUST be a float/double!

alternatively:
http://stackoverflow.com/questions/9843999/calculate-number-of-decimal-places-for-a-float-value-without-libraries.

* Currently internal functionality 				   :noexport:

** =ProcessHistory=: track processing steps

This functionality comprises the =ProcessHistory= class and the =.processHistory=
slot of the =xcmsSet= objects. The =xcmsSet= function already adds a feature
detection processing step for each file to this slot. Subsetting of =xcmsSet=
objects with =[= or =split= correctly process also this slot as does concatenation
using =c=. For processing steps other than /feature detection/ a new element should
be added to the variable =.PROCSTEPS= (defined in /DataClasses.R/.
At some point we could implement methods =getProcessErrors= and =getProcessHistory=
(essentially just calling the =.getProcessErrors= and =.getProcessHistory=
functions in /functions-xcmsSet.R/.

Some additional functionality that could be implemented:
+ Sort the processing history by the =date= slot.
+ Save also analysis properties into an object extending the =ProcessHistory=:
  this would enable to get the exact settings for each processing step.

* Internal changes						   :noexport:

** Changing the way how data is imported

Random errors happen when processing a large number of files with =xcms=. This
might indicate some memory problems, eventually related to the =mzR= package
(similar to the ones spotted in =MSnbase=).

What I want to test:
+ [X] Does =mzR::openMSFile= work also for /netCDF/? No. we would have to check for
  the file type and specify the =backend= based on that.
+ [X] What about writing a new importer that does not need all the objects and
  the presumably old code in =mzR=? -> =readRawData=.

That has been fixed (see above). The /default/ methods for data import form =mzR=
are now used by default.

** Functions and methods to be deprecated and removed.

+ [ ] =xcmsSource= method: not needed anymore, reading is done by =readRawData=.
+ [ ] =loadRaw=, =initialize= for =netCdfSource= and =rampSource=: replaced by
  =readRawData=.
+ [ ] =netCdfSource= and =rampSource= S4 classes: not needed anymore, reading is
  done by =readRawData=.

** Unneeded /R/ files

+ [ ] /netCDF.R/.
+ [ ] /ramp.R/.

*** Unit tests to be removed

+ [ ] /runit.ramp.R/.

* Deprecated functions and files

Here we list all of the functions and related files that are deprecated.

+ =xcmsParallelSetup=, =xcmsPapply=, =xcmsClusterApply=: use =BiocParallel= package
  instead to setup and perform parallel processing, either /via/ the =BPPARAM=
  parameter to function and methods, or by calling =register= to globally set
  parallel processing.

+ =profBin=, =profBinM=, =profBinLin=, =profBinLinM=, =profBinLinBase=, =profBinLinBaseM=:
  replaced by the =binYonX= and =imputeLinInterpol= functions. Also, to create or
  extract the profile matrix from an =xcmsRaw= object, the =profMat= method.


** Deprecated

*** xcms 1.49:

+ =xcmsParallelSetup= (Deprecated.R)
+ =xcmsPapply= (Deprecated.R)
+ =xcmsClusterApply= (Deprecated.R)

*** xcms 1.51:

+ =profBin= (c.R)
+ =profBinM= (c.R)
+ =profBinLin= (c.R)
+ =profBinLinM= (c.R)
+ =profBinLinBase= (c.R)
+ =profBinLinBaseM= (c.R)

** Defunct

* TODOs								   :noexport:

** DONE Deprecate binning functions.
   CLOSED: [2017-02-23 Thu 07:47]

   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
All done except for the retention time correction!!!

** DONE Continue implementing the =do_= functions.
   CLOSED: [2017-02-23 Thu 07:47]
   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
** DONE Define a new object to contain the preprocessing results
   CLOSED: [2017-02-23 Thu 07:47]

   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
This object should replace in the long run the =xcmsSet= object providing the same
functionality while in addition add a better integration of the original raw
data files. The object should contain:

+ Peak/feature data (similar to the =xcmsSet@peaks= slot).
+ Alignment across samples information (similar to the =xcmsSet@groups= slot).
+ Corrected retention time (similar to the =xcmsSet@rt$adjusted= slot).
+ All experimental and phenotypical information.
+ A /link/ to the raw data.
+ History on data manipulation and processing.

Based on these prerequisites, an object extending Biobase's =MSnExp= or
=OnDiskMSnExp= would be ideal. The =MSnExp= would however be /too mighty/ (as it
contains all of the raw data) and the more light weight =OnDiskMSnExp= should
hence be used. While being somewhat similar to the =xcmsSet= =xcmsRaw= object setup,
the new implementation would ensure a better and less error prone import of the
raw (or even processed) data. Some data (TIC etc) are even cached within the
=OnDiskMSnExp= enabling faster data access.

Note that the lack of easy access to raw data disqualifies the =MSnSet= object
from the =MSnbase= package.

The feature data should be placed into the =assayData= environment of the object
to avoid copying etc of the data. Check also =assayDataElement()= in =MSnbase=.

*** Some notes on data usage:
+ Subset by sample: have to extract the corresponding features from the
  features matrix in =assayData= and remove all grouping/alignment
  information. This actually bypasses also the problem to check that feature
  indexes have to be updated.

+ Rename =peaks= to =features=.

+ Better alternative for =groups=: =alignedFeatures=.
+ =groupval=? =featureValues=.

*** Design and implementation:
+ =features= should be still implemented as =matrix= (for performance issues).
+ Alignment information could be implemented as =DataFrame= with the indices added
  to a column =idx=.

** DONE Rename objects, functions and methods
   CLOSED: [2017-02-23 Thu 07:47]

   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
+ [X] =features=: =chromPeaks=.
+ [X] =hasDetectedFeatures=: =hasChromPeaks=.
+ [ ] feature: chromatographic peak.
+ [X] =detectFeatures=: =findChromPeaks=.
+ [X] =dropFeatures=: =dropChromPeaks=.
+ [X] featureDetection-centWave: findChromPeaks-centWave
+ [X] =validFeatureMatrix=: =validChromPeaksMatrix=.

Correspondence.
+ [ ] feature groups: features (aligned and grouped chromatographic peaks).
+ [X] =groupFeatures=: =groupChromPeaks=.
+ [X] =hasAlignedFeatures=: =hasFeatures=.
+ [X] =featureGroups=: =featureDefinitions=, =featureValue= (=groupval=).
+ [X] =FeatureDensityParam=: =PeakDensityParam=.
+ [X] =NearestFeaturesParam=: =NearestPeaksParam=
+ [ ] feature alignment methods: peak alignment methods
+ [X] =$features=: =$chromPeaks=.
+ [X] =featureidx=: =peakidx=.
+ [X] =featureIndex=: =peakIndex=.
+ [X] =dropFeatureGroups=: =dropFeatureDefinitions=.
+ [ ] Peak alignment: Peak grouping
+ [X] =.PROCSTEP.PEAK.ALIGNMENT=: =.PROCSTEP.PEAK.GROUPING=.

Param classes:
+ [X] =extraFeatures=: =extraPeaks=.

RT correction.
+ [X] =featureGroups= retention time correction: =peakGroups=.
+ [X] =FeatureGroupsParam=: =PeakGroupsParam=.
+ [X] =features=: =peaks=
+ [X] =featureIndex=: =peakIndex=
+ [X] =getFeatureGroupsRtMatrix=: =getPeakGroupsRtMatrix=
+ [X] =applyRtAdjToFeatures=: =applyRtAdjToPeaks=.
+ [X] =do_groupFeatures_mzClust=: =do_groupPeaks_mzClust=.

+ [X] Check =maxFeatures= parameter for =do_groupChromPeaks_density=. Is it really
  the maximum number of features, or of peaks?

+ [X] Alignment: retention time correction between samples
  \cite{Sugimoto:2012jt}.
+ [X] Correspondence: (grouping) registration of recurring signals from the same
  analyte over replicate samples \cite{Smith:2014di}.


** TODO Implement the =Chromatogram= class

Now, to accommodate all possibilities:
https://en.wikipedia.org/wiki/Triple_quadrupole_mass_spectrometer
Triple Q-TOF measurements:
+ Product Ion Scan
  - Q1 fixed
  - Q3 scan
+ Precursor Ion Scan
  - Q1 scan
  - Q3 fixed
+ Neutral Loss Scan
  - Q1 scan at mz = m_{product}
  - Q3 scan at mz = m_{product} - m_{neutral molecule}
+ Selected Reaction monitoring (SRM, MRM): Q1 is used to select the precursor
  ion, Q3 cycles through the product ions. Precursor/product pair is referred to
  as a /transition/.
  - Q1 fixed at mz = m_{precursor}
  - Q3 scan at mz = m_{product}


Other resources:
https://en.wikipedia.org/wiki/Mass_chromatogram#Selected-ion_monitoring_chromatogram_.28SIM.29
http://proteowizard.sourceforge.net/dox/structpwiz_1_1msdata_1_1_chromatogram.html
https://sourceforge.net/p/proteowizard/mailman/message/27571266/

*** Move =Chromatogram= to MSnbase

+ [X] Add =Chromatogram= to MSnbase.
+ [ ] Remove =Chromatogram= from xcms.
+ [ ] Move functions and methods to MSnbase.
+ [ ] Fix xcms to import all required stuff from MSnbase.


** TODO Implement a =findBackgroundIons= method

Check on one of our own files.

#+BEGIN_SRC R
  library(xcms)

  rd <- readMSData2("/Volumes/Ext64/data/2016/2016-11/NoSN/250516_QC_NORM_3_POS_3.mzML")

  ## Evaluate the mz-rt matrix - can we spot already something there?
  sps <- spectra(rd)
  dfs <- lapply(sps, as.data.frame)
  ## cut the intensities at 5000
  dfs <- lapply(dfs, function(z) {
      z[z[, "i"] > 5000, "i"] <- 5000
      return(z)
  })

  library(RColorBrewer)
  library(lattice)
  colR <- colorRampPalette(brewer.pal(9, "YlOrRd"))(255)
  brks <- do.breaks(c(0, 5000), length(colR))

  mzR <- range(mz(rd))
  rtR <- range(rtime(rd))

  plot(3, 3, pch = NA, xlim = rtR, ylim = mzR)
  for(i in 1:length(dfs)) {
      intC <- level.colors(dfs[[i]]$i, at = brks, col.regions = colR)
      xs <- rep(rtime(rd)[i], length(intC))
      points(x = xs, y = dfs[[i]]$mz, col = intC, cex = 0.1, pch = 16)
  }
  ## level.colors(x, at = brks, col.regions = colR)
#+END_SRC

A simple approach would be to walk along the mz and evaluate whether, for a
certain mz (bin?) the signal is higher than a threshold in 70% of the spectra,
i.e. that the % of values is larger than a percentage.


** TODO Reduce R CMD check time:

- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: 18m34.630s
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.9.12: 20m41.440s

After tuning xcms:
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: 14m30.454s

After enabling parallel processing for the unit tests:
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: user 21m46.385s

After enabling parallel processing (registering multicoreparam) for the unit 
tests:
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: user 15m53.039s.

tests with long runtime:
+ [ ] testPresentAbsentSumAfterFillPeaks: 13.241
+ [X] test_extractChromatograms (runit.Chromatogram.R): 23.800: Can not reduce
  this.
+ [X] test_obiwarp (runit.do_adjustRtime.R): 17.594: Can not reduce this.
+ [ ] test_findChromPeaks_centWaveWithPredIsoROIs
  (runit.do_findChromPeaks_centWave_isotopes.R): 13.623
+ [X] test_do_groupChromPeaks_nearest (runit.do_groupChromPeaks.R): 25.193: OK.
+ [X] test_fillChromPeaks_matchedFilter (runit.fillChromPeaks.R): 16.843: Can
  not reduce.
+ [X] test.fillPeaks_old_vs_new (runit.fillPeaks.R): 37.924: dontrun
+ [X] test.fillPeaksColumns (runit.fillPeaks.R): 33.552: OK.
+ [X] testFillPeaksPar (runit.fillPeaks.R): 24.752: dontrun
+ [X] test_getEICxset (runit.getEIC.R): 27.144: might be faster.
+ [X] test.getEICretcor (runit.getEIC.R): 17.018: nope.
+ [X] test.issue7 (runit.getEIC.R): 66.020: dontrun
+ [X] test.getXcmsRaw (runit.getXcmsRaw.R): 26.558: might be faster.
+ [X] testMultiFactorDiffreport (runit.phenoData.R): 13.067: nothing to do.




** DONE mzR/MSnbase timings
   CLOSED: [2017-06-14 Wed 11:02]

   - State "DONE"       from "TODO"       [2017-06-14 Wed 11:02]
#+BEGIN_SRC R
  library(MSnbase)
  library(msdata)
  fl <- proteomics(full.names = TRUE)[3]


  ## MSnbase: 2.3.4
  ## mzR: 2.11.2
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.953   0.036   0.986 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.449   0.011   0.460 
  mzR::close(of)

  system.time(tmp <- readMSData2(fl))
  ##  user  system elapsed 
  ## 1.515   0.089   1.596 

  ###########################################
  ## MSnbase: 2.3.4
  ## mzR: 2.11.3
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.974   0.039   1.009 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.422   0.010   0.433 
  mzR::close(of)

  system.time(tmp <- readMSData2(fl))
  ##  user  system elapsed 
  ## 1.509   0.093   1.594 

  fl <- "/Users/jo/data/2016/2016-11/NoSN/190516_POOL_N_POS_14.mzML"
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.138   0.042   0.180 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.067   0.023   0.089 
  mzR::close(of)

  system.time(tmp <- readMSData2(fl))
  ##  user  system elapsed 
  ## 0.708   0.105   0.814 

  ## tmp: 1720 spectra.

  ############################################
  ## MSnbase: 2.3.4
  ## mzR: 2.11.3, without reading the ion injection time
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.969   0.040   1.007 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.449   0.011   0.460 
  mzR::close(of)

  system.time(tmp <- readMSData2(fl))
  ##  user  system elapsed 
  ## 1.556   0.089   1.638 

  fl <- "/Users/jo/data/2016/2016-11/NoSN/190516_POOL_N_POS_14.mzML"
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.138   0.064   0.214 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.065   0.022   0.088 
  mzR::close(of)

  system.time(tmp <- readMSData2(fl))
  ##  user  system elapsed 
  ## 0.709   0.110   0.833 

  ## tmp: 1720 spectra.
#+END_SRC


* References
